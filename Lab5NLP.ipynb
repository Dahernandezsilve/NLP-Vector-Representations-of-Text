{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7f6821",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingeniería - Computación</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> Procesamiento de Lenguaje Natural \n",
    "        <strong>Sección:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 5:</strong> Representaciones Vectoriales de Texto\n",
    " (PPMI, TF-IDF y Word2Vec)</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autor:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hernández Silvestre - <strong>21270</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b722800",
   "metadata": {},
   "source": [
    "Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c2c0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc7694",
   "metadata": {},
   "source": [
    "Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696fed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169 1446 ['rec.autos', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'rec.autos',\n",
    "    'talk.politics.guns',\n",
    "    'talk.politics.mideast',\n",
    "    'talk.politics.misc',\n",
    "]\n",
    "\n",
    "train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True, random_state=42\n",
    ")\n",
    "test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(len(train.data), len(test.data), train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0660673",
   "metadata": {},
   "source": [
    "# Preprocesamiento del corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8049ec",
   "metadata": {},
   "source": [
    "Para determinar qué acciones se tomaran, se decide hacer una exploración inicial del dataset para verificar qué estandarizaciones son útiles y requeridas de realizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdbc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. base='news' → variantes={'news', '\"news\"', 'News.', 'NEWS', 'News', 'News,', 'News:', 'news.', 'News)'}\n",
      "2. base='you' → variantes={'\"You', 'You', 'you!', 'you\"', 'you', 'you.\"', '(You', 'YOU', '**You', '(you,', 'you;', '*you*', 'YOU!\"', 'YOU.', 'you...', 'you:', '\"you', '\"You,', '>you', 'you!)', '=you=', '(you', 'you.', '*you*.', 'YOU,', 'you,\"', 'you,', \"'You\", 'you...............', 'you?\"', 'You,', 'you....', 'you).', '>You', 'you?', '*You*'}\n",
      "3. base='may' → variantes={'may,', 'may.\"', 'May', 'may', 'MAY', '(May'}\n",
      "4. base='have' → variantes={'Have', 'have.', 'have!', '**have', '*have*', 'have', '\"have', 'have?', '(have', 'have,', 'HAVE'}\n",
      "5. base='missed' → variantes={'MISSED,', 'missed'}\n",
      "6. base='apr' → variantes={'Apr', 'APR'}\n",
      "7. base='19' → variantes={'19', '(19).', '19,'}\n",
      "8. base='1993' → variantes={'1993,', '1993:', '1993', '1993.', '1993)'}\n",
      "9. base='not' → variantes={'NOT!)', ':not', '*not*', 'not;', 'not.', 'NOT', '\"Not', 'not?', 'not,', 'not.\"', '(Not', 'not*', 'not)', '(not', 'NOT!', 'NOT!!', 'Not', 'NOT,', 'not\"', '*NOT*', '[not', 'not!', 'not', 'not),'}\n",
      "10. base='because' → variantes={'#Because', 'because:', 'Because', 'Because,', 'because,', 'because', '\"Because', '(because', 'because...\".'}\n",
      "11. base='were' → variantes={'were,', 'Were', 'were', 'WERE', '#were', 'were.', '\"were'}\n",
      "12. base='too' → variantes={'Too', 'too!', 'too;', 'too', 'too)', 'too...', 'too,', 'too.\"', 'too?\"', 'TOO', ':Too', 'too?', 'too!)', 'too..', 'too.'}\n",
      "13. base='but' → variantes={'\"but', '(but', \"'But\", 'But,', 'but...', 'But', '(but,', 'but', 'but:', 'BUT', '[but', '\"But', 'but,'}\n",
      "14. base='in' → variantes={'in.', '(in', ':In', '>in', '*in*,', 'in?', '\"in', 'IN', '#in', 'in...', '[in', 'in,', \"'in\", '[In', '*in', 'in', 'in.\"', ',in', 'In', '(In', '\"In', '>In'}\n",
      "15. base='the' → variantes={'THE', '\"THE', 'The', '(the', '#The', '@>>the', '*The*', 'THE)', '\"The', '#the', '[the', ':The', 'the...', \"'the\", '\"the', '``The', \"''The\", 'the', \"'The\", '\"...The', '**the', '*the*', '~The', 'THe', 'the,', '|>the', '[The', '>The', '--The', '...the', '(The', '>the'}\n",
      "16. base='us' → variantes={'US,', 'US', 'US!', 'us', 'us,', 'us:', 'US\";', 'US\"', 'us!', 'us,\"', '\"us\"', 'us?', 'us?\"', 'US.', 'us.', 'us.\"', 'US)'}\n",
      "17. base='media' → variantes={'\"media', 'media.)', 'media,', 'media.', 'Media,', 'Media', 'media\".', 'media:', 'Media\"', 'media'}\n",
      "18. base='it' → variantes={'>it', 'it,', 'it\",', 'it...).', '*It*', 'it..', 'It', 'it),', '[it', 'it)', 'IT.', 'it?\"', 'it;', 'it,\"', 'it.)', '\"It', '(It', 'it.]', '#>It', \"it.'\", ':It', '#It', '**It', '>It', 'IT', 'it.\"', 'it).', 'it.', 'it!', 'it!)', 'it?', 'it:', 'it\"', 'it'}\n",
      "19. base='those' → variantes={'\"those', '\"Those', 'those', '(Those', 'Those', '\"THOSE', 'THOSE', 'those.'}\n",
      "20. base='intrepid' → variantes={'Intrepid', 'INTREPID'}\n"
     ]
    }
   ],
   "source": [
    "def rawTokenizerWithPunct(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "def normalizeBase(token):\n",
    "    return re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", token.lower())\n",
    "\n",
    "variants = defaultdict(set)\n",
    "\n",
    "for doc in train.data[:1000]:\n",
    "    for token in rawTokenizerWithPunct(doc):\n",
    "        base = normalizeBase(token)\n",
    "        if base:\n",
    "            variants[base].add(token)\n",
    "\n",
    "ambiguous = {k: v for k, v in variants.items() if len(v) > 1}\n",
    "\n",
    "for i, (base, forms) in enumerate(list(ambiguous.items())[:20], 1):\n",
    "    print(f\"{i}. base='{base}' → variantes={forms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19b0fb",
   "metadata": {},
   "source": [
    "Centrandonos en el subconjunto de autor y política, se logró identificar que existen numerosas variaciones superficiales que afectan en la representación de las palabras. Por ejemplo, la palabra news aparece en varias formas como news, News y NEWS, mientras que may se encuentra como may, MAY y may. incluyendo el signo de puntuación. Estas variantes son interpretadas como distintos tokens por los vectorizadores, aunque en la práctica, representan lo mismo.\n",
    "\n",
    "Las variantes identificables corresponden a mayúsculas, minúsculas y signos de puntuación. Por esta razón, se aplica un preprocesamiento que incluye la normalización a minúsculas y la eliminación de signos de puntuación. De esta forma se logrará una mejor representación del contenido existente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dd8d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news',\n",
       " 'you',\n",
       " 'may',\n",
       " 'have',\n",
       " 'missed',\n",
       " 'apr',\n",
       " 'not',\n",
       " 'because',\n",
       " 'you',\n",
       " 'were',\n",
       " 'too',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'because',\n",
       " 'israelists',\n",
       " 'in',\n",
       " 'the',\n",
       " 'us',\n",
       " 'media',\n",
       " 'spiked',\n",
       " 'it',\n",
       " 'those',\n",
       " 'intrepid',\n",
       " 'israeli',\n",
       " 'soldiers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessText(text: str) -> str:\n",
    "    text = text.lower() # minúsculas\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text) # dejar solo letras y espacios (elimina puntuación y dígitos)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # espacios múltiples → uno\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return text.split()\n",
    "\n",
    "trainClean = [preprocessText(doc) for doc in train.data]\n",
    "testClean  = [preprocessText(doc) for doc in test.data]\n",
    "trainTokens = [tokenize(doc) for doc in trainClean]\n",
    "testTokens  = [tokenize(doc) for doc in testClean]\n",
    "\n",
    "trainTokens[0][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2ebff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== RAW ==\n",
      "{'Vocabulary size': 35506, 'Tokens': 700444}\n",
      "\n",
      "== CLEAN ==\n",
      "{'Vocabulary size': 26187, 'Tokens': 526858}\n"
     ]
    }
   ],
   "source": [
    "def statsTokens(docs):\n",
    "    toks = [t for doc in docs for t in doc]\n",
    "    return {\n",
    "        \"Vocabulary size\": len(set(toks)),\n",
    "        \"Tokens\": len(toks),\n",
    "    }\n",
    "\n",
    "def crudeTokens(texts):\n",
    "    return [re.findall(r\"\\w+|\\S\", t) for t in texts] \n",
    "\n",
    "rawDocs = crudeTokens(train.data)\n",
    "rawStats = statsTokens(rawDocs)\n",
    "cleanStats = statsTokens(trainTokens)\n",
    "print(\"== RAW ==\")\n",
    "print({k: v for k, v in rawStats.items() if k != \"top20\"})\n",
    "print(\"\\n== CLEAN ==\")\n",
    "print({k: v for k, v in cleanStats.items() if k != \"top20\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae4225",
   "metadata": {},
   "source": [
    "Se logra observar que la aplicación de la limpieza de la información es efectiva ya que existe una reducción en el tamaño del vocabulario y la cantidad del tokens generados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8fe1a",
   "metadata": {},
   "source": [
    "# Construcción de representación TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f9870",
   "metadata": {},
   "source": [
    "Se busca representar cada documento como un vector de importancia de términos para poder comparar textos y alimentar posteriores modelos. TF-IDF combina cuánto aparece un término el documento (TF) con qué tan raro es en el corpus (IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5672bf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape TRAIN: (2169, 6228)\n",
      "Shape TEST : (1446, 6228)\n",
      "Tamaño de vocabulario: 6228\n",
      "\n",
      "Doc 0 (clase='talk.politics.mideast')\n",
      "   soldiers             0.3405\n",
      "   the                  0.3204\n",
      "   girls                0.1803\n",
      "\n",
      "Doc 1 (clase='talk.politics.guns')\n",
      "   the                  0.2126\n",
      "   effect               0.2010\n",
      "   to                   0.1926\n",
      "\n",
      "Doc 2 (clase='rec.autos')\n",
      "   accident             0.3390\n",
      "   insurance            0.3144\n",
      "   company              0.3101\n",
      "\n",
      "Doc 3 (clase='talk.politics.misc')\n",
      "   tax                  0.3290\n",
      "   income               0.2628\n",
      "   you                  0.2437\n",
      "\n",
      "Doc 4 → [sin términos]\n"
     ]
    }
   ],
   "source": [
    "trainDocs = [\" \".join(toks) for toks in trainTokens]\n",
    "testDocs  = [\" \".join(toks) for toks in testTokens]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,1),\n",
    "    min_df=5,\n",
    "    lowercase=False,                     \n",
    "    token_pattern=r\"(?u)\\b[a-z]{2,}\\b\"  \n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(trainDocs)\n",
    "X_test_tfidf  = tfidf.transform(testDocs)\n",
    "\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "print(\"Shape TRAIN:\", X_train_tfidf.shape)\n",
    "print(\"Shape TEST :\", X_test_tfidf.shape)\n",
    "print(\"Tamaño de vocabulario:\", len(vocab))\n",
    "\n",
    "for doc_id in range(0, 5): # Top términos por documento\n",
    "    row = X_train_tfidf[doc_id]\n",
    "    if row.nnz == 0:  # si el doc quedó vacío con min_df\n",
    "        print(f\"\\nDoc {doc_id} → [sin términos]\")\n",
    "        continue\n",
    "\n",
    "    arr = row.toarray().ravel()\n",
    "    top_idx = arr.argsort()[-3:][::-1]  # top 3\n",
    "    print(f\"\\nDoc {doc_id} (clase='{train.target_names[train.target[doc_id]]}')\")\n",
    "    for i in top_idx:\n",
    "        print(f\"   {vocab[i]:<20} {arr[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80bae9",
   "metadata": {},
   "source": [
    "# Construcción de representación PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d0f8f",
   "metadata": {},
   "source": [
    "#### Matriz de co-ocurrencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6d3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI | vocabSize: 7420\n"
     ]
    }
   ],
   "source": [
    "windowSize = 4       # ventana ±k\n",
    "minCount   = 5       # freq mínima para entrar al vocabulario\n",
    "maxVocab   = 20000   # tope por memoria/tiempo\n",
    "\n",
    "def buildVocab(tokensList, minCount=5, maxVocab=20000):\n",
    "    freq = Counter(t for doc in tokensList for t in doc)\n",
    "    words = [w for w, c in freq.items() if c >= minCount]\n",
    "    words.sort(key=lambda w: -freq[w])\n",
    "    words = words[:maxVocab]\n",
    "    vocab = {w:i for i, w in enumerate(words)}\n",
    "    id2tok = np.array(words)\n",
    "    return vocab, id2tok\n",
    "\n",
    "vocab, id2tok = buildVocab(trainTokens, minCount=minCount, maxVocab=maxVocab)\n",
    "V = len(vocab)\n",
    "print(\"PPMI | vocabSize:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f088de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooc | shape: (7420, 7420) | nnz: 1104492\n"
     ]
    }
   ],
   "source": [
    "def buildCoocCsr(tokensList, vocab, windowSize=4):\n",
    "    rows, cols, data = [], [], []\n",
    "    for doc in tokensList:\n",
    "        idxs = [vocab[t] for t in doc if t in vocab]\n",
    "        L = len(idxs)\n",
    "        for i, wi in enumerate(idxs):\n",
    "            s = max(0, i - windowSize)\n",
    "            e = min(L, i + windowSize + 1)\n",
    "            for j in range(s, e):\n",
    "                if j == i: \n",
    "                    continue\n",
    "                rows.append(wi)\n",
    "                cols.append(idxs[j])\n",
    "                data.append(1.0)\n",
    "    C = coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float64)\n",
    "    return C.tocsr()\n",
    "\n",
    "C = buildCoocCsr(trainTokens, vocab, windowSize=windowSize)\n",
    "print(\"Cooc | shape:\", C.shape, \"| nnz:\", C.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be1b71",
   "metadata": {},
   "source": [
    "### Calcule la matriz PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ead8da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI | shape: (7420, 7420) | nnz: 1104492\n"
     ]
    }
   ],
   "source": [
    "def computePpmiCsr(C: csr_matrix, useLog2=True, eps=1e-12):\n",
    "    total = C.sum()\n",
    "    sumW  = np.array(C.sum(axis=1)).ravel()   # totales por palabra (filas)\n",
    "    sumC  = np.array(C.sum(axis=0)).ravel()   # totales por contexto (columnas)\n",
    "\n",
    "    Ccoo = C.tocoo(copy=True)                 # trabajamos sobre los no-cero\n",
    "    pWc  = Ccoo.data / total\n",
    "    pW   = sumW[Ccoo.row] / total\n",
    "    pC   = sumC[Ccoo.col] / total\n",
    "\n",
    "    logFn = np.log2 if useLog2 else np.log\n",
    "    pmi   = logFn((pWc + eps) / (pW * pC + eps))\n",
    "    pmi[pmi < 0] = 0.0                        # PPMI = max(PMI, 0)\n",
    "\n",
    "    PPMI = coo_matrix((pmi, (Ccoo.row, Ccoo.col)), shape=C.shape).tocsr()\n",
    "    return PPMI\n",
    "\n",
    "PPMI = computePpmiCsr(C, useLog2=True)       # log2 como en la slide\n",
    "print(\"PPMI | shape:\", PPMI.shape, \"| nnz:\", PPMI.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdcc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs PPMI | train: (2169, 7420) | test: (1446, 7420)\n"
     ]
    }
   ],
   "source": [
    "def docsToPpmiDocs(tokensList, vocab, ppmiCsr):\n",
    "    V = ppmiCsr.shape[0]\n",
    "    X = np.zeros((len(tokensList), V), dtype=np.float32)\n",
    "    for i, doc in enumerate(tokensList):\n",
    "        idxs = [vocab[t] for t in doc if t in vocab]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        # promedio de las filas PPMI correspondientes a las palabras del Qdoc\n",
    "        X[i] = ppmiCsr[idxs].mean(axis=0).A1\n",
    "    return X\n",
    "\n",
    "PPMI_train_docs = docsToPpmiDocs(trainTokens, vocab, PPMI)\n",
    "PPMI_test_docs  = docsToPpmiDocs(testTokens,  vocab, PPMI)\n",
    "print(\"Docs PPMI | train:\", PPMI_train_docs.shape, \"| test:\", PPMI_test_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b80b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top contextos por PPMI\n",
      "→ gun\n",
      "strict               6.0626\n",
      "melbourne            5.9922\n",
      "defenses             5.8403\n",
      "prevalence           5.8402\n",
      "strictest            5.8402\n",
      "stricter             5.5772\n",
      "zones                5.5772\n",
      "ownership            5.5292\n",
      "zip                  5.4397\n",
      "control              5.4111\n",
      "→ car\n",
      "favorite             5.5602\n",
      "cruisers             5.5056\n",
      "crawl                5.4661\n",
      "sports               5.4354\n",
      "swap                 5.3537\n",
      "museums              5.3537\n",
      "screwing             5.3537\n",
      "collections          5.3537\n",
      "overturned           5.2606\n",
      "wax                  5.1108\n"
     ]
    }
   ],
   "source": [
    "def topContexts(word, k=10):\n",
    "    if word not in vocab:\n",
    "        print(f\"'{word}' no está en el vocab.\"); return\n",
    "    i = vocab[word]\n",
    "    row = PPMI[i].toarray().ravel()\n",
    "    top = row.argsort()[-k:][::-1]\n",
    "    for j in top:\n",
    "        if row[j] > 0:\n",
    "            print(f\"{id2tok[j]:<20} {row[j]:.4f}\")\n",
    "\n",
    "print(\"\\nTop contextos por PPMI\")\n",
    "print(\"→ gun\")\n",
    "topContexts(\"gun\", 10)\n",
    "print(\"→ car\")\n",
    "topContexts(\"car\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad0f0f",
   "metadata": {},
   "source": [
    "# Construcción de representación Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7408e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabSize (w2v): 7420\n",
      "vectorSize: 200\n"
     ]
    }
   ],
   "source": [
    "def setSeed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def trainWord2Vec(sentences, vectorSize=200, window=5, minCount=5, sg=1, workers=4, seed=42):\n",
    "    \"\"\"\n",
    "    Entrena Word2Vec (gensim) sobre listas de tokens.\n",
    "    - sentences: lista de listas de tokens (trainTokens)\n",
    "    \"\"\"\n",
    "    setSeed(seed)\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vectorSize,\n",
    "        window=window,\n",
    "        min_count=minCount,\n",
    "        sg=sg,                 # 1 = skip-gram, 0 = CBOW\n",
    "        workers=workers,\n",
    "        negative=10,           # negative sampling\n",
    "        epochs=5,\n",
    "        seed=seed\n",
    "    )\n",
    "    return model\n",
    "\n",
    "w2vModel = trainWord2Vec(trainTokens, vectorSize=200, window=5, minCount=5, sg=1)\n",
    "print(\"vocabSize (w2v):\", len(w2vModel.wv))\n",
    "print(\"vectorSize:\", w2vModel.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7214af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrainW2V shape: (2169, 200) | XTestW2V shape: (1446, 200)\n"
     ]
    }
   ],
   "source": [
    "def docEmbedding(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def docsToEmbeddings(tokensList, model):\n",
    "    X = np.zeros((len(tokensList), model.vector_size), dtype=np.float32)\n",
    "    for i, toks in enumerate(tokensList):\n",
    "        X[i] = docEmbedding(toks, model)\n",
    "    return X\n",
    "\n",
    "XTrainW2V = docsToEmbeddings(trainTokens, w2vModel)\n",
    "XTestW2V  = docsToEmbeddings(testTokens,  w2vModel)\n",
    "print(\"XTrainW2V shape:\", XTrainW2V.shape, \"| XTestW2V shape:\", XTestW2V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a457141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Most similar words (car) ---------------\n",
      "tires                0.8471\n",
      "engine               0.8230\n",
      "manual               0.8165\n",
      "test                 0.8160\n",
      "dealer               0.8158\n",
      "taurus               0.8110\n",
      "rear                 0.8110\n",
      "ford                 0.8096\n",
      "pedal                0.8066\n",
      "honda                0.8013\n",
      "--------------- Least similar words (car) ---------------\n",
      "azerbaijan           -0.0369\n",
      "international        -0.0054\n",
      "anti                 0.0045\n",
      "soviet               0.0155\n",
      "against              0.0162\n",
      "leaders              0.0239\n",
      "rights               0.0494\n",
      "union                0.0642\n",
      "by                   0.0645\n",
      "united               0.0662\n"
     ]
    }
   ],
   "source": [
    "def mostSimilarWords(model, word, topn=10):\n",
    "    if word not in model.wv:\n",
    "        print(f\"'{word}' no está en el vocabulario del modelo\")\n",
    "        return\n",
    "    for w, sim in model.wv.most_similar(word, topn=topn):\n",
    "        print(f\"{w:<20} {sim:.4f}\")\n",
    "\n",
    "def leastSimilarWords(model, word, k=10, minFreq=50):\n",
    "    \"\"\"\n",
    "    Busca palabras 'lejanas' a 'word' entre términos con frecuencia >= minFreq.\n",
    "    Nota: es meramente ilustrativo; la 'lejanía' puede estar sesgada por rareza.\n",
    "    \"\"\"\n",
    "    if word not in model.wv:\n",
    "        print(f\"'{word}' no está en el vocabulario del modelo\")\n",
    "        return\n",
    "    target = model.wv[word]\n",
    "    # candidatos frecuentes\n",
    "    cand = [w for w in model.wv.index_to_key if model.wv.get_vecattr(w, \"count\") >= minFreq and w != word]\n",
    "    if not cand:\n",
    "        print(\"No hay candidatos con esa frecuencia mínima\")\n",
    "        return\n",
    "    sims = np.array([model.wv.similarity(word, w) for w in cand])\n",
    "    idx = np.argsort(sims)[:k]   # los k más bajos\n",
    "    for i in idx:\n",
    "        print(f\"{cand[i]:<20} {sims[i]:.4f}\")\n",
    "\n",
    "# ejemplos típicos del dataset:\n",
    "print(\"--------------- Most similar words (car) ---------------\")\n",
    "mostSimilarWords(w2vModel, \"car\", topn=10)\n",
    "print(\"--------------- Least similar words (car) ---------------\")\n",
    "leastSimilarWords(w2vModel, \"car\", k=10, minFreq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0af3340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes finales de documentos:\n",
      "TF-IDF: (2169, 26161)\n",
      "PPMI: (2169, 7420)\n",
      "Word2Vec: (2169, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShapes finales de documentos:\")\n",
    "print(\"TF-IDF:\", X_train_tfidf.shape)\n",
    "print(\"PPMI:\", PPMI_train_docs.shape)\n",
    "print(\"Word2Vec:\", XTrainW2V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba3e2",
   "metadata": {},
   "source": [
    "# Evaluación comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66871fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEval(X_train, y_train, X_test, y_test, name):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03adca56",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9522c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: 0.7635\n"
     ]
    }
   ],
   "source": [
    "accTfidf = trainAndEval(X_train_tfidf, train.target, X_test_tfidf, test.target, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd52a5",
   "metadata": {},
   "source": [
    "### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a02bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI (SVD): 0.7607\n"
     ]
    }
   ],
   "source": [
    "accPpmi = trainAndEval(PPMI_train_docs, train.target, PPMI_test_docs, test.target, \"PPMI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb18359",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fbb5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: 0.6902\n"
     ]
    }
   ],
   "source": [
    "accW2v = trainAndEval(XTrainW2V, train.target, XTestW2V, test.target, \"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99d3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados comparativos:\n",
      "  Representación  Accuracy\n",
      "0         TF-IDF  0.763485\n",
      "1     PPMI (SVD)  0.760719\n",
      "2  Word2Vec mean  0.690180\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Representación\": [\"TF-IDF\", \"PPMI (SVD)\", \"Word2Vec mean\"],\n",
    "    \"Accuracy\": [accTfidf, accPpmi, accW2v]\n",
    "})\n",
    "print(\"\\nResultados comparativos:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
